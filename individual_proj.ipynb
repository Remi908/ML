{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define TrajectoryDataset class\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, dataframe, window_length=100):\n",
    "        # Normalize the data\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        normalized_df = pd.DataFrame(self.scaler.fit_transform(dataframe), columns=dataframe.columns)\n",
    "\n",
    "        # Apply custom transformation for windowing\n",
    "        sliced_df = self.custom_transformation(normalized_df.to_numpy(), window_length=window_length)\n",
    "\n",
    "        # Convert to tensor for PyTorch\n",
    "        self.data = torch.tensor(sliced_df, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def custom_transformation(self, dataframe_array, window_length):\n",
    "        # Windowing to create inputs and targets\n",
    "        window_length += 1  # Extra column for the target\n",
    "        sliced_data = np.lib.stride_tricks.sliding_window_view(dataframe_array, window_shape=(window_length,), axis=1)\n",
    "        sliced_data = sliced_data.reshape(-1, window_length)\n",
    "        return sliced_data\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        # Apply inverse transformation to get back original scale\n",
    "        return self.scaler.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the LSTM-based model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer to produce final output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])  # Last time step output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def autoregressive_predict(model, initial_input, prediction_length, device):\n",
    "    \"\"\"\n",
    "    Perform autoregressive prediction using the trained LSTM model.\n",
    "    \n",
    "    Args:\n",
    "    - model: The trained PyTorch model.\n",
    "    - initial_input: The last sequence of time steps used as input for prediction, shaped [1, sequence_length, input_size].\n",
    "    - prediction_length: The number of future time steps to predict.\n",
    "    - device: The device (CPU or GPU) for computation.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: A list of predicted values over the prediction_length.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    \n",
    "    # Move initial input to the device\n",
    "    current_input = initial_input.to(device)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for prediction\n",
    "        for _ in range(prediction_length):\n",
    "            # Perform a forward pass to get the next prediction\n",
    "            output = model(current_input)\n",
    "            \n",
    "            # Save the prediction (output is [1, output_size])\n",
    "            next_value = output.item()  # Convert to scalar if output_size=1\n",
    "            predictions.append(next_value)\n",
    "            \n",
    "            # Update current_input by removing the first time step and appending the prediction\n",
    "            next_input = torch.cat((current_input[:, 1:, :], output.unsqueeze(0).unsqueeze(-1)), dim=1)\n",
    "            current_input = next_input  # Update for the next prediction\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the uploaded files\n",
    "train_path = 'train.csv'\n",
    "val_path = 'val.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "# Load the dataframes\n",
    "train_df = pd.read_csv(train_path, header=0).drop('ids', axis=1, errors='ignore')\n",
    "val_df = pd.read_csv(val_path, header=0).drop('ids', axis=1, errors='ignore')\n",
    "test_df = pd.read_csv(test_path, header=0).drop('ids', axis=1, errors='ignore')\n",
    "\n",
    "# Initialize dataset with normalization and windowing\n",
    "window_length = 100  # Adjust as needed for your model\n",
    "train_dataset = TrajectoryDataset(dataframe=train_df, window_length=window_length)\n",
    "val_dataset = TrajectoryDataset(dataframe=val_df, window_length=window_length)\n",
    "test_dataset = TrajectoryDataset(dataframe=test_df, window_length=window_length)\n",
    "\n",
    "# Extract validation set data as a tensor from the val_dataset\n",
    "val_set = torch.tensor(val_df.values[:,:].astype(np.float32), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameter tuning\n",
    "from itertools import product\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define ranges for hyperparameters\n",
    "hidden_sizes = [32, 64, 128]\n",
    "num_layers_list = [1, 2, 3]\n",
    "learning_rates = [0.001, 0.01]\n",
    "batch_sizes = [8, 16, 32]\n",
    "\n",
    "# Variable to track the best performance\n",
    "best_mse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for hidden_size, num_layers, learning_rate, batch_size in product(hidden_sizes, num_layers_list, learning_rates, batch_sizes):\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size=1, hidden_size=hidden_size, num_layers=num_layers, output_size=1).to(device)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create DataLoader for the training dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model for a few epochs (e.g., 3) for quick evaluation\n",
    "    num_epochs_tuning = 3\n",
    "    for epoch in range(num_epochs_tuning):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs = data[:, :-1].reshape(-1, 100, 1).to(device)\n",
    "            targets = data[:, -1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    initial_input_val = val_set[:, -window_length:].reshape(1, -1, 1).to(device)\n",
    "    full_trajectories = autoregressive_predict(model, initial_input_val, prediction_length=val_set.shape[1], device=device)\n",
    "    \n",
    "    # Calculate MSE for validation predictions\n",
    "    mse = criterion(full_trajectories, val_set).item()\n",
    "    \n",
    "    # Check if this is the best combination\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_params = (hidden_size, num_layers, learning_rate, batch_size)\n",
    "        print(f\"New best MSE: {best_mse:.4f} with params: hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "best_hidden_size=best_params[0]\n",
    "best_num_layers=best_params[1]\n",
    "best_learning_rate=best_params[2]\n",
    "best_batch_size=best_params[3]\n",
    "#Print the best hyperparameters found\n",
    "print(f\"Best parameters found: hidden_size={best_params[0]}, num_layers={best_params[1]}, learning_rate={best_params[2]}, batch_size={best_params[3]}\")\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model hyperparameters\n",
    "# input_size = 1              # Each time step has a single feature (the value at that time step)\n",
    "# hidden_size = 64            # Number of units in the LSTM layer\n",
    "# num_layers = 2              # Number of LSTM layers\n",
    "# output_size = 1             # Predicting the next value in sequence\n",
    "num_epochs = 5             # Number of training epochs\n",
    "\n",
    "# Re-initialize and train the model using the best hyperparameters\n",
    "model = LSTMModel(input_size=1, hidden_size=best_hidden_size, num_layers=best_num_layers, output_size=1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "accumulation_steps = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs = data[:, :-1].reshape(-1, 100, 1).to(device)\n",
    "        targets = data[:, -1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Only update weights every 'accumulation_steps' batches\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the last 100 time steps from the validation set as the initial input\n",
    "initial_input = val_dataset.data[-1, :-1].reshape(1, 100, 1)  # Shape to [1, 100, 1]\n",
    "\n",
    "# Number of future steps to predict\n",
    "prediction_length = 10  # Predict the next 10 time steps\n",
    "\n",
    "# Perform autoregressive prediction\n",
    "predicted_sequence = autoregressive_predict(model, initial_input, prediction_length, device)\n",
    "\n",
    "# Display predicted sequence\n",
    "print(\"Predicted future values:\", predicted_sequence)\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "\n",
    "train_set = torch.tensor(train_df.values[:,:].astype(np.float32), dtype=torch.float32)\n",
    "val_set = torch.tensor(val_df.values[:,:].astype(np.float32), dtype=torch.float32)\n",
    "test_set = torch.tensor(val_df.values[:,:].astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "points_to_predict = val_set.shape[1]\n",
    "\n",
    "# Autoregressive prediction function\n",
    "def autoregressive_predict(model, input_maxtrix, prediction_length=points_to_predict):\n",
    "    \"\"\"\n",
    "    Perform autoregressive prediction using the learned model.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained PyTorch model.\n",
    "    - input_maxtrix: A matrix of initial time steps (e.g., shape (963, window_length)).\n",
    "    - prediction_length: The length of the future trajectory to predict.\n",
    "\n",
    "    Returns:\n",
    "    - output_matrix: A tensor of the predicted future trajectory of the same length as `prediction_length`.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    output_matrix = torch.empty(input_maxtrix.shape[0],0)\n",
    "    current_input = input_maxtrix\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients for prediction\n",
    "        for idx in range(prediction_length):\n",
    "            # Predict the next time step\n",
    "            next_pred = model(current_input)\n",
    "\n",
    "            # Concatenating the new column along dimension 1 (columns)\n",
    "            output_matrix = torch.cat((output_matrix, next_pred), dim=1)\n",
    "\n",
    "            # Use the predicted value as part of the next input\n",
    "            current_input = torch.cat((current_input[:, 1:],next_pred),dim=1)\n",
    "\n",
    "    return output_matrix\n",
    "\n",
    "\n",
    "initial_input = train_set[:, -window_length:]  #use the last window of training set as initial input\n",
    "full_trajectories = autoregressive_predict(model, initial_input)\n",
    "\n",
    "\n",
    "# Calculate MSE between predicted trajectories and actual validation trajectories using torch\n",
    "mse_loss = MSELoss()\n",
    "\n",
    "# Compute MSE\n",
    "mse = mse_loss(full_trajectories, val_set)\n",
    "\n",
    "# Print MSE\n",
    "print(f'Autoregressive Validation MSE (using torch): {mse.item():.4f}')\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform autoregressive predictions for one row in the validation set\n",
    "# We can pick a specific row (e.g., row 0) to visualize\n",
    "row_idx = 0  # You can change this to visualize predictions for different rows\n",
    "initial_input = val_set[row_idx, :window_length].unsqueeze(0)\n",
    "\n",
    "# Predict future trajectory of length 100\n",
    "predicted_trajectory = autoregressive_predict(model, initial_input)\n",
    "\n",
    "# Get the actual trajectory for comparison\n",
    "actual_trajectory = val_set[row_idx].numpy()\n",
    "\n",
    "# Plot the actual vs predicted trajectory\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(actual_trajectory)), actual_trajectory, label=\"Actual Trajectory\", color='blue', marker='o')\n",
    "plt.plot(range(len(actual_trajectory)), predicted_trajectory.squeeze().numpy(), label=\"Predicted Trajectory\", color='red', linestyle='--', marker='x')\n",
    "plt.title(f\"Actual vs Predicted Trajectory (Row {row_idx})\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all the validation dataset\n",
    "initial_input = train_set[:, -window_length:]\n",
    "val_predictions_tensor = autoregressive_predict(model, initial_input)\n",
    "\n",
    "# Generate predictions for all the test dataset\n",
    "initial_input = val_predictions_tensor[:, -window_length:]\n",
    "test_predictions_tensor = autoregressive_predict(model, initial_input)\n",
    "\n",
    "\n",
    "# Print their shapes\n",
    "print(f'Validation Predictions Tensor Shape: {val_predictions_tensor.shape}')\n",
    "print(f'Test Predictions Tensor Shape: {test_predictions_tensor.shape}')\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submissions_v4(pred_val_tensor, pred_test_tensor, original_val_path, original_test_path):\n",
    "    # Read the original validation and testing datasets\n",
    "    original_val_df = pd.read_csv(original_val_path)\n",
    "    original_test_df = pd.read_csv(original_test_path)\n",
    "\n",
    "    # Ensure the shape of pred_val_tensor and pred_test_tensor is correct\n",
    "    assert pred_val_tensor.shape[0] * pred_val_tensor.shape[1] == original_val_df.shape[0] * (original_val_df.shape[1] - 1)\n",
    "    assert pred_test_tensor.shape[0] * pred_test_tensor.shape[1] == original_test_df.shape[0] * (original_test_df.shape[1] - 1)\n",
    "\n",
    "    # Create empty lists to store ids and values\n",
    "    ids = []\n",
    "    values = []\n",
    "\n",
    "    # Process validation set\n",
    "    for col_idx, col in enumerate(original_val_df.columns[1:]):  # Skip the 'ids' column\n",
    "        for row_idx, _ in enumerate(original_val_df[col]):\n",
    "            ids.append(str(f\"{col}_traffic_val_{row_idx}\"))\n",
    "            values.append(float(pred_val_tensor[row_idx, col_idx]))\n",
    "\n",
    "    # Process testing set\n",
    "    for col_idx, col in enumerate(original_test_df.columns[1:]):  # Skip the 'ids' column\n",
    "        for row_idx, _ in enumerate(original_test_df[col]):\n",
    "            ids.append(str(f\"{col}_traffic_test_{row_idx}\"))\n",
    "            values.append(float(pred_test_tensor[row_idx, col_idx]))\n",
    "\n",
    "    # Create the submissions dataframe\n",
    "    submissions_df = pd.DataFrame({\n",
    "        \"ids\": ids,\n",
    "        \"value\": values\n",
    "    })\n",
    "\n",
    "    # Impute any null values\n",
    "    submissions_df.fillna(100, inplace=True)\n",
    "\n",
    "    # Assert the shape of the dataframe\n",
    "    assert submissions_df.shape[1] == 2\n",
    "    assert submissions_df.shape[0] == (original_val_df.shape[0] * (original_val_df.shape[1] - 1)) + (original_test_df.shape[0] * (original_test_df.shape[1] - 1))\n",
    "    assert \"ids\" in submissions_df.columns\n",
    "    assert \"value\" in submissions_df.columns\n",
    "\n",
    "    # Save to CSV\n",
    "    submissions_df.to_csv('submissions_v3.csv', index=False)\n",
    "\n",
    "# Call the function\n",
    "generate_submissions_v4(val_predictions_tensor, test_predictions_tensor, '/kaggle/input/cse-575-project-2/val.csv', '/kaggle/input/cse-575-project-2/test.csv')\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
